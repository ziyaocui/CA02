{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ziyao Cui naive_bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "outputId": "659a08ae-89ed-4297-91ae-3ce3115e1aa0"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "#import all useful packages here \n",
        "drive.mount('/content/drive/')\n",
        "#import packages and mounted to read files from dirve"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "#clean up the data\n",
        "def make_Dictionary(root_dir):              #create a function called make_Dictionary\n",
        "  all_words = []                            #create a new empty list called all_words\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] \n",
        "  #create a new list called emails, it join root_dir and f using os.path.join, f is in the list of root_dir\n",
        "  #os.path.join() means join one or more path together\n",
        "  #os.listdir() will print out all files in the path \n",
        "  for mail in emails: \n",
        "    with open(mail) as m:                   #reading mail in the emails list, open each mail and call it m \n",
        "      for line in m:\n",
        "        words = line.split()                #Then split each word in line in the mail and call the list words\n",
        "        all_words += words                  #Then add words into all_words\n",
        "  dictionary = Counter(all_words)           #Counter is a dictionary subclass that counting hashable object, so it will count frequency in all_words and store it to dictionary\n",
        "  list_to_remove = list(dictionary)         #make a list of dictionary and call the list list_to_remove\n",
        "\n",
        "  for item in list_to_remove:               #for any item in the list list_to_remove\n",
        "    if item.isalpha() == False:             #if the item is not equal to alpha\n",
        "      del dictionary[item]                  #then delete it from the dictionary item\n",
        "    elif len(item) == 1:                    #else, if length of the item is equal to one\n",
        "      del dictionary[item]                  #also delete it\n",
        "  dictionary = dictionary.most_common(3000) #using most_common return a list of most common 3000 words in the dictionary and store it back to dictionary\n",
        "  return dictionary                         #output is the dictionary, and will delete words that are useless for analyze like 'i, he, are'\n",
        "            "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "#feature extraction\n",
        "def extract_features(mail_dir):                                   #create a function called extract_features\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]   #create a new list called files, it join mail_dir and fi using os.path.join, fi is in the list of root_dir\n",
        "  features_matrix = np.zeros((len(files),3000))                   #use np.zeros to return an array of zeros with rows (equal to the length of files) and 3000 columns and store the list to feature_matrix\n",
        "  train_labels = np.zeros(len(files))                             #create an new array of zeros, only contain columns in the list and column is equal to the length of files and store the list to train_labels\n",
        "  count = 1;                                                      #start counting from 1\n",
        "  docID = 0;                                                      #docID starts from 0\n",
        "  for fil in files: \n",
        "    with open(fil) as fi:                                         #reading fil in the files list first, open each fil and call it fi \n",
        "      for i, line in enumerate(fi):                               #use enumerate() will give you the count and value, so count fi\n",
        "        if i ==2:                                                 #if i is equal to 2 which means body of email is at the third line of the text file\n",
        "          words = line.split()                                    #then split the line and store the list to words\n",
        "          for word in words:                                      #each word in list words, it starts from 0\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(dictionary):                    #reading i, d from dictionary\n",
        "              if d[0] == word:  \n",
        "                wordID = i                                        #if the first item in d is word, then wordID is equal to 1 \n",
        "                features_matrix[docID,wordID] = words.count(word) #put docID and wordID back into feature_matrix and count word\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')                             #a new list filepathTokens is to split fil with '/'\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]           #lastToken contains less 1 than filepathTokens\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "        train_labels[docID] = 1; \n",
        "        count = count + 1\n",
        "      docID = docID + 1                                           #if the lastToken start with 'spmsg', docID for train_labels stars at 1 and add 1 each time\n",
        "  return features_matrix, train_labels                            #the output is feature_matrix and train labels, create a dictionary with words and frequency, each row represents one message and column is word frequency\n",
        "                                                                  #in order to check if the occurence of this word is in the dictionary           "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/test-mails'\n",
        "#read files from google drive"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "3c8cb66a-5ba6-42b2-b607-bd1226a6031a"
      },
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB() #we are using Gaussian Naive Bayes model here\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels) #use the Gaussian model to fit feature_matrix\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix) #use Gaussian model to predict test_features_matrix\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))\n",
        "#the accuracy score is more than 96% which means this model is very accurate and it gives us a good prediction."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}